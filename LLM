from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    pipeline
)
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
from trl import SFTTrainer
from huggingface_hub import login, HfApi
from accelerate import Accelerator
import os
import json
import torch

# Initialize Accelerator at the top, right after imports
accelerator = Accelerator()
device = accelerator.device

print("Successfully imported required libraries for dataset handling, model configuration, and LoRA fine-tuning.")
login(token="")


try:
    api = HfApi()
    user_info = api.whoami()
    print(f"Token validated successfully! Logged in as: {user_info['name']}")
except Exception as e:
    print(f"Token validation failed. Error: {e}")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_ID   = "meta-llama/Llama-3.3-70B-Instruct"
DATA_PATH  = "../6_Reasoning_fine_tune_dataset.jsonl"
OUTPUT_DIR = "./results_lora2"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. TOKENIZER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
tok.pad_token = tok.eos_token
tok.padding_side = "right"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. BASE MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16,
    # device_map="auto",  # âœ… Auto-distributes model across available GPUs
)
base.config.use_cache = False
base.config.pretraining_tp = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. LORA CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

model = get_peft_model(base, lora_cfg)
model.train()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. SANITY CHECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.print_trainable_parameters()
assert any(p.requires_grad for p in model.parameters()), "âŒ No trainable parameters found!"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. DATA LOADING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ds = load_dataset("json", data_files=DATA_PATH, split="train")

def tokenize(example):
    prompt = tok.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False
    )
    encoded = tok(
        prompt,
        truncation=True,
        max_length=4096,
        padding="max_length",
        return_tensors="pt"
    )
    input_ids = encoded["input_ids"].squeeze()
    attention_mask = encoded["attention_mask"].squeeze()
    labels = input_ids.clone()
    labels[attention_mask == 0] = -100
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

train_ds = ds.map(tokenize, remove_columns=["messages"])
eval_dataset = train_ds.shuffle(seed=42).select(range(500))

collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Token Length Diagnostic â”€â”€â”€â”€â”€â”€â”€â”€â”€
avg_len = sum(len(x['input_ids']) for x in train_ds) / len(train_ds)
print(f"ğŸ§  Avg token length in dataset: {avg_len:.2f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. TRAINING ARGS â”€â”€â”€â”€â”€â”€â”€â”€â”€
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    learning_rate=4e-5,
    bf16=True,
    save_strategy="steps",
    save_steps=50,
    eval_steps=50,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    report_to="tensorboard",
    remove_unused_columns=False,
    gradient_checkpointing=False,
    dataloader_pin_memory=True,
    dataloader_num_workers=2,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. TRAINER SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€
trainer = Trainer(
    model=model,
    tokenizer=tok,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_dataset,
    data_collator=collator
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. TRAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
trainer.train()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. SAVE ADAPTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.eval()
final_dir = f"{OUTPUT_DIR}/final_adapter"
model.save_pretrained(final_dir)
tok.save_pretrained(final_dir)
with open(os.path.join(final_dir, "training_args.json"), "w") as f:
    json.dump(args.to_dict(), f, indent=2)

print(f"\nâœ… LoRA fine-tune complete â€” adapter saved to: {final_dir}")


